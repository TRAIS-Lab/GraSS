{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nodes(file_path):\n",
    "    int_list = []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        for row in csv_reader:\n",
    "            for item in row:\n",
    "                try:\n",
    "                    int_list.append(int(item))\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: '{item}' could not be converted to an integer and was skipped.\")\n",
    "    return int_list\n",
    "\n",
    "def calculate_one(path):\n",
    "    # score = torch.load(path, map_location=torch.device('cpu'))  # _test_0225_regroup\n",
    "    score = torch.load(path, map_location=torch.device('cpu'))\n",
    "    # score = torch.rand(5000, 500)\n",
    "    print(\"score shape:\", score.shape)\n",
    "\n",
    "    nodes_str = []\n",
    "    for i in range(50):\n",
    "        nodes_str.append(f\"./checkpoints/{i}/train_index.csv\")\n",
    "\n",
    "    full_nodes = [i for i in range(4656)]\n",
    "\n",
    "    node_list = []\n",
    "    for node_str in nodes_str:\n",
    "        numbers = read_nodes(node_str)\n",
    "        index = []\n",
    "        for number in numbers:\n",
    "            index.append(full_nodes.index(number))\n",
    "        node_list.append(index)\n",
    "\n",
    "    loss_list = torch.load(\"./result/gt.pt\", map_location=torch.device('cpu')).detach()\n",
    "\n",
    "    approx_output = []\n",
    "    for i in range(len(nodes_str)):\n",
    "        score_approx_0 = score[node_list[i], :]\n",
    "        sum_0 = torch.sum(score_approx_0, axis=0)\n",
    "        approx_output.append(sum_0)\n",
    "\n",
    "    print(len(loss_list), loss_list[0].shape)\n",
    "    print(len(approx_output), approx_output[0].shape)\n",
    "\n",
    "    res = 0\n",
    "    counter = 0\n",
    "    for i in range(score.shape[1]):\n",
    "        tmp = spearmanr(np.array([approx_output[k][i] for k in range(len(approx_output))]),\n",
    "                        np.array([loss_list[k][i].numpy() for k in range(len(loss_list))])).statistic\n",
    "        if np.isnan(tmp):\n",
    "            print(\"Numerical issue\")\n",
    "            continue\n",
    "        res += tmp\n",
    "        counter += 1\n",
    "\n",
    "    print(counter)\n",
    "\n",
    "    return res/counter, loss_list, approx_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3899744/1026402471.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  score = torch.load(path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score shape: torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3899744/1026402471.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loss_list = torch.load(\"./result/gt.pt\", map_location=torch.device('cpu')).detach()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3155 is out of bounds for dimension 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./result/score.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcalculate_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mcalculate_one\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     35\u001b[0m approx_output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(nodes_str)):\n\u001b[0;32m---> 37\u001b[0m     score_approx_0 \u001b[38;5;241m=\u001b[39m \u001b[43mscore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     38\u001b[0m     sum_0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(score_approx_0, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     39\u001b[0m     approx_output\u001b[38;5;241m.\u001b[39mappend(sum_0)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3155 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "source": [
    "path = \"./result/score.pt\"\n",
    "print(calculate_one(path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score shape: torch.Size([100, 10])\n",
      "tensor([20.5490, 28.3675, 27.1684, 22.0018])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3899744/1346549292.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  score = torch.load('./result/score_FJLT-512.pt', map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "score = torch.load('./result/score_FJLT-512.pt', map_location=torch.device('cpu'))\n",
    "# score = torch.rand(5000, 500)\n",
    "print(\"score shape:\", score.shape)\n",
    "print(score[0:4, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score shape: torch.Size([100, 10])\n",
      "tensor([20.5487, 28.3678, 27.1683, 22.0017])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3899744/669972308.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  score = torch.load('./result/score_FJLT-512_threshold-1e-07.pt', map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "score = torch.load('./result/score_FJLT-512_threshold-1e-07.pt', map_location=torch.device('cpu'))\n",
    "# score = torch.rand(5000, 500)\n",
    "print(\"score shape:\", score.shape)\n",
    "print(score[0:4, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score shape: torch.Size([4, 1])\n",
      "tensor([[0.0656],\n",
      "        [0.0806],\n",
      "        [0.0821],\n",
      "        [0.0708]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3899744/818564332.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  score = torch.load('./result/score_FJLT-512_GIP.pt', map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "score = torch.load('./result/score_FJLT-512_GIP.pt', map_location=torch.device('cpu'))\n",
    "# score = torch.rand(5000, 500)\n",
    "print(\"score shape:\", score.shape)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score shape: torch.Size([4, 1])\n",
      "tensor([[-0.0019],\n",
      "        [ 0.0029],\n",
      "        [ 0.0151],\n",
      "        [ 0.0002]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3899744/2181850493.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  score = torch.load('./result/score.pt', map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "score = torch.load('./result/score.pt', map_location=torch.device('cpu'))\n",
    "# score = torch.rand(5000, 500)\n",
    "print(\"score shape:\", score.shape)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0338    , 0.04152866, 0.04230152, 0.36479268])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.0656, 0.0806, 0.0821, 0.708])\n",
    "\n",
    "x * (0.0338 / 0.0656)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 123614976\n",
      "Total number of parameters: 123614976\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def compute_total_parameters(layers_string):\n",
    "    # Regular expression to extract in_features, out_features, and bias information\n",
    "    pattern = r'GCLinear\\(in_features=(\\d+), out_features=(\\d+), bias=(\\w+)\\)'\n",
    "    matches = re.findall(pattern, layers_string)\n",
    "\n",
    "    total_parameters = 0\n",
    "    for in_features, out_features, bias in matches:\n",
    "        in_features = int(in_features)\n",
    "        out_features = int(out_features)\n",
    "        # Compute the parameters for weights\n",
    "        params = in_features * out_features\n",
    "        # Add bias parameters if bias=True\n",
    "        if bias == \"True\":\n",
    "            params += out_features\n",
    "        total_parameters += params\n",
    "\n",
    "    return total_parameters\n",
    "\n",
    "# Example usage\n",
    "layers_string = \"Trainable layers: [GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=50257, bias=False)]\"\n",
    "\n",
    "layers_string_2 = \"Trainable layers: [GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=2304, bias=True), GCLinear(in_features=768, out_features=768, bias=True), GCLinear(in_features=768, out_features=3072, bias=True), GCLinear(in_features=3072, out_features=768, bias=True), GCLinear(in_features=768, out_features=50257, bias=False)]\"\n",
    "\n",
    "total_params = compute_total_parameters(layers_string)\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "total_params = compute_total_parameters(layers_string_2)\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123717120"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "124439808\n",
    "\n",
    "123614976 + 768+2304+768+768+3072+768+768+2304+768+768+3072+768+768+2304+768+768+3072+768+768+2304+768+768+3072+768+768+2304+768+768+3072+768+768+2304+768+768+3072+768+768+2304+768+768+3072+768+768+2304+768+768+3072+768+768+2304+768+768+3072+768+768+2304+768+768+3072+768+768+2304+768+768+3072+768+768+2304+768+768+3072+768+768\n",
    "\n",
    "# 124439808 + 38597376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of matrix elements: 163037184\n"
     ]
    }
   ],
   "source": [
    "def calculate_total_matrix_size(matrix_string):\n",
    "    \"\"\"\n",
    "    Calculate the total number of elements across all matrices in the given string.\n",
    "\n",
    "    Args:\n",
    "    matrix_string (str): A string containing matrix sizes in torch.Size format\n",
    "\n",
    "    Returns:\n",
    "    int: Total number of elements across all matrices\n",
    "    \"\"\"\n",
    "    # Use regex to extract numbers inside the torch.Size brackets\n",
    "    matrix_sizes = re.findall(r'torch\\.Size\\(\\[([^\\]]+)\\]', matrix_string)\n",
    "\n",
    "    # Convert the extracted strings to actual size tuples\n",
    "    total_elements = 0\n",
    "    for size_str in matrix_sizes:\n",
    "        # Split by comma and convert to integers, stripping any whitespace\n",
    "        size_tuple = tuple(map(int, size_str.split(',')))\n",
    "\n",
    "        # Calculate the number of elements in this matrix\n",
    "        matrix_elements = 1\n",
    "        for dim in size_tuple:\n",
    "            matrix_elements *= dim\n",
    "\n",
    "        total_elements += matrix_elements\n",
    "\n",
    "    return total_elements\n",
    "\n",
    "\n",
    "# Example usage\n",
    "matrix_string = \"[torch.Size([50257, 768]), torch.Size([1024, 768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([2304, 768]), torch.Size([2304]), torch.Size([768, 768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([3072, 768]), torch.Size([3072]), torch.Size([768, 3072]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([50257, 768])]\"\n",
    "total_size = calculate_total_matrix_size(matrix_string)\n",
    "print(f\"Total number of matrix elements: {total_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "unbalanced parenthesis at position 25",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_elements\n\u001b[1;32m     35\u001b[0m string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.wte.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([50257, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.wpe.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([1024, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.0.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.1.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.2.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.3.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.4.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.5.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.6.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.7.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.8.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.9.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.10.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.ln_1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.ln_1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.attn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.attn.c_attn.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([2304])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.attn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.attn.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.ln_2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.ln_2.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.mlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072, 768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.mlp.c_fc.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768, 3072])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.11.mlp.c_proj.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.ln_f.weight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768])), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.ln_f.bias\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, torch.Size([768]))]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 37\u001b[0m total_size \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_total_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal number of bias elements: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 12\u001b[0m, in \u001b[0;36mcalculate_total_size\u001b[0;34m(input_string)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mCalculate the total size of matrix elements.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mint: Total number of elements\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Extract torch.Size values\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m size_matches \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m.Size\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m[([^\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m]]+)\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate total elements\u001b[39;00m\n\u001b[1;32m     15\u001b[0m total_elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/IF/lib/python3.10/re.py:240\u001b[0m, in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfindall(string)\n",
      "File \u001b[0;32m~/miniconda3/envs/IF/lib/python3.10/re.py:303\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sre_compile\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 303\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msre_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IF/lib/python3.10/sre_compile.py:788\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[1;32m    787\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[0;32m--> 788\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43msre_parse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    790\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IF/lib/python3.10/sre_parse.py:969\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munbalanced parenthesis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mgrouprefpos:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mgroups:\n",
      "\u001b[0;31merror\u001b[0m: unbalanced parenthesis at position 25"
     ]
    }
   ],
   "source": [
    "def calculate_total_size(input_string):\n",
    "    \"\"\"\n",
    "    Calculate the total size of matrix elements.\n",
    "\n",
    "    Args:\n",
    "    input_string (str): A string containing parameter names and their torch.Size\n",
    "\n",
    "    Returns:\n",
    "    int: Total number of elements\n",
    "    \"\"\"\n",
    "    # Extract torch.Size values\n",
    "    size_matches = re.findall(r'torch\\.Size\\(\\[([^\\]]+)\\])', input_string)\n",
    "\n",
    "    # Calculate total elements\n",
    "    total_elements = 0\n",
    "\n",
    "    # Iterate through the extracted size strings\n",
    "    for size_str in size_matches:\n",
    "        # Split the size string and convert to integers\n",
    "        dims = [int(dim.strip()) for dim in size_str.split(',')]\n",
    "\n",
    "        # Calculate size based on number of dimensions\n",
    "        if len(dims) == 1:\n",
    "            # Single dimensional size\n",
    "            total_elements += dims[0]\n",
    "        else:\n",
    "            # Multi-dimensional size - multiply all dimensions\n",
    "            size_product = 1\n",
    "            for dim in dims:\n",
    "                size_product *= dim\n",
    "            total_elements += size_product\n",
    "\n",
    "    return total_elements\n",
    "\n",
    "string = \"[('transformer.wte.weight', torch.Size([50257, 768])), ('transformer.wpe.weight', torch.Size([1024, 768])), ('transformer.h.0.ln_1.weight', torch.Size([768])), ('transformer.h.0.ln_1.bias', torch.Size([768])), ('transformer.h.0.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.0.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.0.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.0.attn.c_proj.bias', torch.Size([768])), ('transformer.h.0.ln_2.weight', torch.Size([768])), ('transformer.h.0.ln_2.bias', torch.Size([768])), ('transformer.h.0.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.0.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.0.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.0.mlp.c_proj.bias', torch.Size([768])), ('transformer.h.1.ln_1.weight', torch.Size([768])), ('transformer.h.1.ln_1.bias', torch.Size([768])), ('transformer.h.1.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.1.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.1.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.1.attn.c_proj.bias', torch.Size([768])), ('transformer.h.1.ln_2.weight', torch.Size([768])), ('transformer.h.1.ln_2.bias', torch.Size([768])), ('transformer.h.1.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.1.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.1.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.1.mlp.c_proj.bias', torch.Size([768])), ('transformer.h.2.ln_1.weight', torch.Size([768])), ('transformer.h.2.ln_1.bias', torch.Size([768])), ('transformer.h.2.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.2.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.2.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.2.attn.c_proj.bias', torch.Size([768])), ('transformer.h.2.ln_2.weight', torch.Size([768])), ('transformer.h.2.ln_2.bias', torch.Size([768])), ('transformer.h.2.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.2.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.2.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.2.mlp.c_proj.bias', torch.Size([768])), ('transformer.h.3.ln_1.weight', torch.Size([768])), ('transformer.h.3.ln_1.bias', torch.Size([768])), ('transformer.h.3.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.3.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.3.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.3.attn.c_proj.bias', torch.Size([768])), ('transformer.h.3.ln_2.weight', torch.Size([768])), ('transformer.h.3.ln_2.bias', torch.Size([768])), ('transformer.h.3.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.3.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.3.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.3.mlp.c_proj.bias', torch.Size([768])), ('transformer.h.4.ln_1.weight', torch.Size([768])), ('transformer.h.4.ln_1.bias', torch.Size([768])), ('transformer.h.4.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.4.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.4.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.4.attn.c_proj.bias', torch.Size([768])), ('transformer.h.4.ln_2.weight', torch.Size([768])), ('transformer.h.4.ln_2.bias', torch.Size([768])), ('transformer.h.4.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.4.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.4.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.4.mlp.c_proj.bias', torch.Size([768])), ('transformer.h.5.ln_1.weight', torch.Size([768])), ('transformer.h.5.ln_1.bias', torch.Size([768])), ('transformer.h.5.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.5.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.5.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.5.attn.c_proj.bias', torch.Size([768])), ('transformer.h.5.ln_2.weight', torch.Size([768])), ('transformer.h.5.ln_2.bias', torch.Size([768])), ('transformer.h.5.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.5.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.5.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.5.mlp.c_proj.bias', torch.Size([768])), ('transformer.h.6.ln_1.weight', torch.Size([768])), ('transformer.h.6.ln_1.bias', torch.Size([768])), ('transformer.h.6.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.6.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.6.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.6.attn.c_proj.bias', torch.Size([768])), ('transformer.h.6.ln_2.weight', torch.Size([768])), ('transformer.h.6.ln_2.bias', torch.Size([768])), ('transformer.h.6.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.6.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.6.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.6.mlp.c_proj.bias', torch.Size([768])), ('transformer.h.7.ln_1.weight', torch.Size([768])), ('transformer.h.7.ln_1.bias', torch.Size([768])), ('transformer.h.7.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.7.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.7.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.7.attn.c_proj.bias', torch.Size([768])), ('transformer.h.7.ln_2.weight', torch.Size([768])), ('transformer.h.7.ln_2.bias', torch.Size([768])), ('transformer.h.7.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.7.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.7.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.7.mlp.c_proj.bias', torch.Size([768])), ('transformer.h.8.ln_1.weight', torch.Size([768])), ('transformer.h.8.ln_1.bias', torch.Size([768])), ('transformer.h.8.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.8.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.8.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.8.attn.c_proj.bias', torch.Size([768])), ('transformer.h.8.ln_2.weight', torch.Size([768])), ('transformer.h.8.ln_2.bias', torch.Size([768])), ('transformer.h.8.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.8.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.8.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.8.mlp.c_proj.bias', torch.Size([768])), ('transformer.h.9.ln_1.weight', torch.Size([768])), ('transformer.h.9.ln_1.bias', torch.Size([768])), ('transformer.h.9.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.9.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.9.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.9.attn.c_proj.bias', torch.Size([768])), ('transformer.h.9.ln_2.weight', torch.Size([768])), ('transformer.h.9.ln_2.bias', torch.Size([768])), ('transformer.h.9.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.9.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.9.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.9.mlp.c_proj.bias', torch.Size([768])), ('transformer.h.10.ln_1.weight', torch.Size([768])), ('transformer.h.10.ln_1.bias', torch.Size([768])), ('transformer.h.10.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.10.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.10.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.10.attn.c_proj.bias', torch.Size([768])), ('transformer.h.10.ln_2.weight', torch.Size([768])), ('transformer.h.10.ln_2.bias', torch.Size([768])), ('transformer.h.10.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.10.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.10.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.10.mlp.c_proj.bias', torch.Size([768])), ('transformer.h.11.ln_1.weight', torch.Size([768])), ('transformer.h.11.ln_1.bias', torch.Size([768])), ('transformer.h.11.attn.c_attn.weight', torch.Size([2304, 768])), ('transformer.h.11.attn.c_attn.bias', torch.Size([2304])), ('transformer.h.11.attn.c_proj.weight', torch.Size([768, 768])), ('transformer.h.11.attn.c_proj.bias', torch.Size([768])), ('transformer.h.11.ln_2.weight', torch.Size([768])), ('transformer.h.11.ln_2.bias', torch.Size([768])), ('transformer.h.11.mlp.c_fc.weight', torch.Size([3072, 768])), ('transformer.h.11.mlp.c_fc.bias', torch.Size([3072])), ('transformer.h.11.mlp.c_proj.weight', torch.Size([768, 3072])), ('transformer.h.11.mlp.c_proj.bias', torch.Size([768])), ('transformer.ln_f.weight', torch.Size([768])), ('transformer.ln_f.bias', torch.Size([768]))]\"\n",
    "\n",
    "total_size = calculate_total_size(string)\n",
    "print(f\"\\nTotal number of bias elements: {total_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
