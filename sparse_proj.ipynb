{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*torch.load.*weights_only=False.*\")\n",
    "\n",
    "import time\n",
    "\n",
    "import SJLT\n",
    "\n",
    "import torch\n",
    "from _dattri.utlis import compute_pairwise_distance_metrics, compute_pairwise_inner_product_rank_correlation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# First, check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection_dims = [256, 512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "projection_dims = [256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "# sparsity_levels = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "sparsity_levels = [0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "def test_random_projection_quality(projection_dims, sparsity_levels=sparsity_levels, mode=\"SJLT\", activation_fn=\"relu\", c=20, blow_up=1):\n",
    "    # Store results for each sparsity level\n",
    "    relative_errors_dist = {sparsity: [] for sparsity in sparsity_levels}\n",
    "    spearman_correlations = {sparsity: [] for sparsity in sparsity_levels}\n",
    "    times = {sparsity: [] for sparsity in sparsity_levels}\n",
    "    overhead_times = {sparsity: [] for sparsity in sparsity_levels}\n",
    "    computation_times = {sparsity: [] for sparsity in sparsity_levels}\n",
    "    memory_usage = {sparsity: [] for sparsity in sparsity_levels}  # For memory usage\n",
    "\n",
    "    for sparsity in sparsity_levels:\n",
    "        print(f\"\\tSparsity: {sparsity}\")\n",
    "        # load the data from torch.save(grad_t, f\"grad_t_{ckpt_idx}_{train_batch_idx}.pt\") with ckpt_idx = 0 and train_batch_idx = 0 to 9\n",
    "        for i in range(10):\n",
    "            grad_t = torch.load(f\"result/grad/{activation_fn}/grad_t_0_{i}.pt\", map_location=device)\n",
    "            if i == 0:\n",
    "                batch_vec = grad_t\n",
    "            else:\n",
    "                batch_vec = torch.cat([batch_vec, grad_t], dim=0)\n",
    "\n",
    "        batch_size, original_dim = batch_vec.size()\n",
    "\n",
    "        # Sparsify the vectors by randomly setting a fraction (sparsity) of elements to zero\n",
    "        num_elements_to_drop = int(sparsity * original_dim)\n",
    "        if num_elements_to_drop > 0:\n",
    "            for i in range(batch_size):\n",
    "                indices_to_drop = torch.randperm(original_dim)[:num_elements_to_drop]\n",
    "                batch_vec[i, indices_to_drop] = 0\n",
    "\n",
    "        for proj_dim in projection_dims:\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            print(f\"\\t\\tProjection dimension: {proj_dim}\")\n",
    "\n",
    "            # Start timing for overhead\n",
    "            torch.cuda.synchronize()\n",
    "            overhead_start_time = time.time()\n",
    "\n",
    "            if mode in [\"SJLT\", \"SJLT_batch\"]:\n",
    "                rand_indices = torch.randint(proj_dim * blow_up, (original_dim, c), device=device)\n",
    "                rand_signs = torch.randint(0, 2, (original_dim, c), device=device) * 2 - 1\n",
    "            elif mode == \"SJLT_reverse\":\n",
    "                rand_indices = torch.randint(proj_dim, (original_dim, c), device=device)\n",
    "                rand_signs = torch.randint(0, 2, (original_dim, c), device=device) * 2 - 1\n",
    "                pos_indices, neg_indices = SJLT.backward_SJLT_indices(original_dim, proj_dim, c, device, rand_indices, rand_signs)\n",
    "            else:\n",
    "                proj_matrix = torch.randn(proj_dim, original_dim, device=batch_vec.device) / (proj_dim ** 0.5)\n",
    "\n",
    "            # End timing for overhead\n",
    "            torch.cuda.synchronize()\n",
    "            overhead_end_time = time.time()\n",
    "\n",
    "            # Start timing for the actual projection\n",
    "            torch.cuda.synchronize()\n",
    "            computation_start_time = time.time()\n",
    "\n",
    "            if mode == \"SJLT\":\n",
    "                batch_vec_p = SJLT.SJLT(batch_vec, proj_dim, rand_indices=rand_indices, rand_signs=rand_signs, c=20, blow_up=blow_up)\n",
    "            elif mode == \"SJLT_batch\":\n",
    "                batch_vec_p = SJLT.SJLT_batch(batch_vec, proj_dim, rand_indices=rand_indices, rand_signs=rand_signs, c=20, blow_up=blow_up)\n",
    "            elif mode == \"SJLT_reverse\":\n",
    "                batch_vec_p = SJLT.SJLT_reverse(batch_vec, proj_dim, pos_indices=pos_indices, neg_indices=neg_indices, c=20)\n",
    "            else:\n",
    "                batch_vec_p = batch_vec @ proj_matrix.T\n",
    "\n",
    "            # End timing for the actual projection\n",
    "            torch.cuda.synchronize()\n",
    "            computation_end_time = time.time()\n",
    "\n",
    "            # Calculate relative error for the current projection dimension\n",
    "            relative_error = compute_pairwise_distance_metrics(batch_vec, batch_vec_p)\n",
    "            relative_errors_dist[sparsity].append(relative_error)\n",
    "\n",
    "            # Calculate Spearman rank correlation for inner products\n",
    "            spearman_corr = compute_pairwise_inner_product_rank_correlation(batch_vec, batch_vec_p)\n",
    "            spearman_correlations[sparsity].append(spearman_corr)\n",
    "\n",
    "            # Record times\n",
    "            overhead_time = overhead_end_time - overhead_start_time\n",
    "            computation_time = computation_end_time - computation_start_time\n",
    "            total_time = overhead_time + computation_time\n",
    "\n",
    "            overhead_times[sparsity].append(overhead_time)\n",
    "            computation_times[sparsity].append(computation_time)\n",
    "            times[sparsity].append(total_time)\n",
    "\n",
    "            peak_mem = torch.cuda.max_memory_allocated()\n",
    "            memory_usage[sparsity].append(peak_mem / 1024**3)  # Store peak memory in GB\n",
    "\n",
    "    # Updated plotting for six graphs\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(14, 18))  # Create a 3x2 grid for subplots\n",
    "\n",
    "    # Flatten axes for easier indexing\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Relative Errors\n",
    "    ax = axes[0]\n",
    "    for sparsity in sparsity_levels:\n",
    "        ax.plot(projection_dims, relative_errors_dist[sparsity], marker='o', label=f'Sparsity: {sparsity}')\n",
    "    ax.set_xticks(projection_dims)\n",
    "    ax.set_title(\"Relative Error Between Pairwise Distances vs. Projection Dimension\")\n",
    "    ax.set_xlabel(\"Projection Dimension\")\n",
    "    ax.set_ylabel(\"Relative Error (Mean)\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Spearman Correlations\n",
    "    ax = axes[1]\n",
    "    for sparsity in sparsity_levels:\n",
    "        ax.plot(projection_dims, spearman_correlations[sparsity], marker='o', label=f'Sparsity: {sparsity}')\n",
    "    ax.set_xticks(projection_dims)\n",
    "    ax.set_title(\"Spearman Rank Correlation of Pairwise Inner Products vs. Projection Dimension\")\n",
    "    ax.set_xlabel(\"Projection Dimension\")\n",
    "    ax.set_ylabel(\"Spearman Rank Correlation\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Overhead Times\n",
    "    ax = axes[2]\n",
    "    for sparsity in sparsity_levels:\n",
    "        ax.plot(projection_dims, overhead_times[sparsity], marker='o', label=f'Sparsity: {sparsity}')\n",
    "    ax.set_xticks(projection_dims)\n",
    "    ax.set_title(\"Overhead Time vs. Projection Dimension\")\n",
    "    ax.set_xlabel(\"Projection Dimension\")\n",
    "    ax.set_ylabel(\"Overhead Time (seconds)\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Computation Times\n",
    "    ax = axes[3]\n",
    "    for sparsity in sparsity_levels:\n",
    "        ax.plot(projection_dims, computation_times[sparsity], marker='o', label=f'Sparsity: {sparsity}')\n",
    "    ax.set_xticks(projection_dims)\n",
    "    ax.set_title(\"Computation Time vs. Projection Dimension\")\n",
    "    ax.set_xlabel(\"Projection Dimension\")\n",
    "    ax.set_ylabel(\"Computation Time (seconds)\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Total Computation Times\n",
    "    ax = axes[4]\n",
    "    for sparsity in sparsity_levels:\n",
    "        ax.plot(projection_dims, times[sparsity], marker='o', label=f'Sparsity: {sparsity}')\n",
    "    ax.set_xticks(projection_dims)\n",
    "    ax.set_title(\"Total Time vs. Projection Dimension\")\n",
    "    ax.set_xlabel(\"Projection Dimension\")\n",
    "    ax.set_ylabel(\"Total Time (seconds)\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Memory Usage\n",
    "    ax = axes[5]\n",
    "    for sparsity in sparsity_levels:\n",
    "        ax.plot(projection_dims, memory_usage[sparsity], marker='o', label=f'Sparsity: {sparsity}')\n",
    "    ax.set_xticks(projection_dims)\n",
    "    ax.set_title(\"Peak GPU Memory Usage vs. Projection Dimension\")\n",
    "    ax.set_xlabel(\"Projection Dimension\")\n",
    "    ax.set_ylabel(\"Peak Memory Usage (GB)\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation function: relu\n",
      "\tSparsity: 0\n",
      "\t\tProjection dimension: 256\n",
      "\t\tProjection dimension: 512\n"
     ]
    }
   ],
   "source": [
    "for activation_fn in [\"relu\", \"tanh\", \"sigmoid\", \"leaky_relu\", \"linear\"]:\n",
    "    print(f\"Activation function: {activation_fn}\")\n",
    "    test_random_projection_quality(projection_dims, mode=\"SJLT\", activation_fn=activation_fn, c=20, blow_up=1)\n",
    "    test_random_projection_quality(projection_dims, mode=\"SJLT_batch\", activation_fn=activation_fn, c=20, blow_up=1)\n",
    "    test_random_projection_quality(projection_dims, mode=\"SJLT_reverse\", activation_fn=activation_fn, c=20, blow_up=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
